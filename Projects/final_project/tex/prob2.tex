\section*{Problem No.2} \label{sec:prob2}



\paragraph{Part A:}
Here we explain in details who to derive the matrix-form formulation. We have $n$ date points, for each, we have $x_{i} \in \mathbb{R}^{11}$ features and $y_{i}\in \mathbb{R}$ label. The optimization problem is to find $a$  and $b$ such that 

$$
\argmin_{a\in \mathbb{R}^{11}, b\in \mathbb{R}} \frac{1}{n} \sum_{i=1}^{n}|a^{T}x_{i}+b - y_{i}|
$$

We define $\omega \in \mathbb{R}^{12}$ as concatenation (in MATLAB language) of $b$ on top of $a$. Similarly, we define  $\tilde{x} \in \mathbb{R}^{12}$  by concatenating 1 on top of $x$ as follows
\[
\omega \in \mathbb{R}^{12} =
\left[
\begin{array}{c}
b \\
a_{1} \\
a_{2} \\
...\\
...\\
a_{10} \\
a_{11} \\
\end{array} 
\right]
\qquad \qquad
\tilde{x} \in \mathbb{R}^{12} =
\left[
\begin{array}{c}
1 \\
x_{1} \\
x_{2} \\
...\\
...\\
x_{10} \\
x_{11} \\
\end{array} 
\right]
\]

We can use $\omega$ and $\tilde{x}$ to redefine the optimization as follows
\[
\argmin_{w\in \mathbb{R}^{12}} \frac{1}{n} \sum_{i=1}^{n}|w^{T}\tilde{x}_{i} - y_{i}|
\]

This can be written in matrix form by defining the matrix $X\in \mathbb{R}^{13 \times n}$ and vector $W \in \mathbb{R}^{13}$ as follows (superscript emphasizes different data points)

$$
X\in \mathbb{R}^{13 \times n} = 
\left[
\begin{array}{cc ccc c}
\tilde{x}_{0}^{1} =1 &  \tilde{x}_{0}^{2} =1 & \cdots & \cdots & \cdots &  \tilde{x}_{0}^{n} =1\\
\tilde{x}_{1}^{1} = x_{1}^{1} & \tilde{x}_{1}^{2} = x_{1}^{2} & \cdots & \cdots & \cdots & \tilde{x}_{1}^{n} = x_{1}^{n}\\
\tilde{x}_{2}^{1} = x_{2}^{1} & \tilde{x}_{2}^{2} = x_{2}^{2} & \cdots & \cdots & \cdots & \tilde{x}_{2}^{n} = x_{2}^{n}\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots\\
\tilde{x}_{11}^{1} = x_{11}^{1} & \tilde{x}_{11}^{2} = x_{11}^{2} & \cdots & \cdots & \cdots & \tilde{x}_{11}^{n} = x_{11}^{n}\\
-y^{1} & -y^{2} & \cdots & \cdots & \cdots & -y^{n}\\
\end{array} 
\right],
W \in \mathbb{R}^{13} =
\left[
\begin{array}{c}
b \\
a_{1} \\
a_{2} \\
...\\
...\\
a_{10} \\
a_{11} \\
1
\end{array} 
\right]
$$

Then the optimization problem can be re-written as 
\[
\argmin_{W\in \mathbb{R}^{13}} |X^{T}W|
\]

Minimizing the 1-norm of a vector can be formulated as an Linear Programming (LP) problem as follows

\[
\begin{array}{cl}
\text{minimize} &  \mathbf{1}^{T}S\\
\text{subject to} & X^{T}W \preceq S \\
				  & -X^{T}W \preceq S
\end{array} 
\]
where $S\in \mathbb{R}^{n}$. From that, we can define the variables in the question to be: $c = \mathbf{1} \in \mathbb{R}^{1}$, $A = [X -X]^{T} \in \mathbb{R}^{2n\times 13} $, $d = S\in \mathbb{R}^{13}$, and $b = W\in \mathbb{R}^{13}$.


\paragraph{Part B:}
To solve the LP problem, we used \texttt{cvx}; MATLAB-based modeling system for convex optimization. \texttt{cvx} makes it easy to write and code the problem as a mathematical equations without much conversion compared with using \texttt{linprog} within MATLAB Optimization toolbox. 

The optimal value we get was \textbf{0.49375}. The value represents the average absolute error of the linear model using the provided data set. Since the score (or labels) varies from 0 to 10, the values 0.49375 looks acceptable compared with the range of the scores.

The following shows the parameters of the model
\begin{footnotesize}
\mathleft
\[
a= \text{[}\quad 0.083682\qquad -0.84129\qquad -0.23035\qquad 0.061643\qquad -1.608 \quad \cdots\cdots
\]

\[
\qquad \qquad \qquad \cdots \quad 0.0018835\qquad -0.0027255\qquad -61.517\qquad -0.041654\qquad 1.0888\qquad 0.29696 \text{]}
\]
\[
b = 63.1391
\]
\end{footnotesize}


\paragraph{Part C:}
We used \texttt{cvx} to solve the least-squares regression problem by replacing the 1-norm by 2-norm. The model we get is as follows 
\begin{footnotesize}
\mathleft
\[
a= \text{[}\quad 0.025086 \qquad -1.0835 \qquad -0.18256 \qquad 0.016374 \qquad -1.8741\quad \cdots\cdots
\]

\[
\qquad \qquad  \cdots \quad 0.0043605 \qquad -0.0032643 \qquad -17.9835 \qquad -0.41315 \qquad 0.91647 \qquad 0.2761  \text{]}
\]
\[
b = 22.0655
\]
\end{footnotesize}
The residual sum of squared errors (RSS) is \textbf{666.4107}. Note that  RSS = $\sum_{i=1}^{n}(y_{i}-a^{T}x_{i}-b)^{2}$.  %Note that in computing RSS, we did not divide by $n$ (number of data points) however the objective function that we used for minimization divides the RSS by $n$ i.e., $min\ \frac{1}{n} \sqrt{\sum_{i=1}^{n}(y_{i}-a^{T}x_{i}-b)^{2}}$.

\paragraph{Part D:}
Here we implemented the LASSO model by adding a regularization term to the least-squares regression model. The regularization is $\lambda |q|_{1}$ where $q\in \mathbb{R}^12 = [a;b]$ (i.e., extends the $a$ vector to contain $b$ as well). We experimented with few values for $\lambda$ in order to reach a value that will turn most of the elements of $q$ to zero except four of them; those that determines the quality of wine. With $\lambda = 0.2$, we found the four features are the top four: \emph{volatile acidity}, \emph{sulphates}, \emph{pH}, and \emph{alcohol}. Our model is as follows 


\begin{footnotesize}
\mathleft
\[
a= \text{[}\quad 0.063692  \qquad  -0.92564 \qquad -5.3119\times 10^{-5}  \qquad   1.2099\times 10^{-5} \qquad -8.1434 \times 10^{-6} \quad  \cdots\cdots
\]

\[
\qquad \qquad  \cdots \quad 0.0044334    \qquad -0.0024063 \qquad   0.00044866       \qquad  0.38007 \qquad    0.66368 \qquad  0.32965 \text{]}
\]
\[
b = 0.5012
\]
\end{footnotesize}



\textbf{Note:} In order to run the code, \texttt{cvx} should be installed on your machine. 